name = "Google Gemini"
url = "https://aistudio.google.com/"

local U = ...
if not U.restapi then return end
exe = true

historyName = name

predefined = {
  api_base={
    "https://generativelanguage.googleapis.com/v1beta",
    "https://generativelanguage.googleapis.com/v1alpha",
    "https://generativelanguage.googleapis.com/v1"
  },
  model="gemini-2.0-flash",
  apikey=win.GetEnv"GEMINI_API_KEY",
  responseMimeType={
    "",
    "text/plain", -- (default)
    "application/json",
    "text/x.enum", --https://ai.google.dev/gemini-api/docs/file-prompting-strategies#plain_text_formats
  },
  HarmBlockThreshold={ --https://ai.google.dev/api/generate-content#HarmBlockThreshold
    "",
    "HARM_BLOCK_THRESHOLD_UNSPECIFIED",	--Threshold is unspecified.
    "BLOCK_LOW_AND_ABOVE",	--Content with NEGLIGIBLE will be allowed.
    "BLOCK_MEDIUM_AND_ABOVE",	--Content with NEGLIGIBLE and LOW will be allowed.
    "BLOCK_ONLY_HIGH",	--Content with NEGLIGIBLE, LOW, and MEDIUM will be allowed.
    "BLOCK_NONE",	--All content will be allowed.
    "OFF",	--Turn off the safety filter.
  },
  tools={
    "",
    --https://ai.google.dev/gemini-api/docs/code-execution?lang=rest
    "codeExecution:{}",
    "googleSearch:{}",
    --https://ai.google.dev/gemini-api/docs/grounding?lang=rest
    --[[
    "{googleSearchRetrieval={
       dynamicRetrievalConfig={
         mode: MODE_UNSPECIFIED|MODE_DYNAMIC
         dynamicThreshold: number
       },
     }",
    --]]
  },
}

--https://ai.google.dev/api/generate-content#BlockReason
--https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters
local function formatCategory (safetyRatings)
  local out = {}
  for _,rating in ipairs(safetyRatings) do
    if rating.blocked then
      out[#out+1] = ("%s: %s"):format(rating.category, rating.probability)
    end
  end
  if #out==0 then return "" end
  return (" (%s)"):format(table.concat(out, ", "))
end

local function wrapCode (data)
  if data.outcome and data.outcome~="OUTCOME_OK" then
    return data.outcome
  end
  return ("```%s\n%s\n```"):format(data.language or "output", data.code or data.output)
end

local function processNonText (part)
  local execData = part.executableCode or part.codeExecutionResult
  if execData then
    --https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/code-execution-api
    --https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/code-execution
    return wrapCode(execData)
  else --todo
    --https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/function-calling#rest-openai
    --https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#examples_of_function_calling
    return "Error: unknown part's type\n"..U.formatJson(part)
  end
end

local function getMsgText (data)
  local msg = data.message or data.error or data
  msg = msg.error or msg
  return msg.message or msg
end

--https://ai.google.dev/gemini-api/docs/safety-settings
local HarmCategory = { --https://ai.google.dev/api/generate-content#harmcategory
  --"HARM_CATEGORY_UNSPECIFIED",	--Category is unspecified.
  "HARM_CATEGORY_HARASSMENT",	--Harasment content.
  "HARM_CATEGORY_HATE_SPEECH",	--Hate speech and content.
  "HARM_CATEGORY_SEXUALLY_EXPLICIT",	--Sexually explicit content.
  "HARM_CATEGORY_DANGEROUS_CONTENT",	--Dangerous content.
  "HARM_CATEGORY_CIVIC_INTEGRITY",	--Content that may be used to harm civic integrity.
}
--https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages
return function (session, stream, context, api_base, apikey, model, temperature, thinkingBudget, role,
                 maxOutputTokens, candidateCount, responseMimeType, responseSchema, seed, stopSequences, tools,
                 top_p, top_k, presencePenalty, frequencyPenalty, responseLogprobs, logprobs, HarmBlockThreshold)
  local number,obj,boolean = U.number, U.obj, U.boolean
  local client = U.restapi.GoogleGemini(apikey, api_base)

  local safetySettings --https://ai.google.dev/api/generate-content#safetysetting
  if HarmBlockThreshold then
    safetySettings = {}
    for i,cat in ipairs(HarmCategory) do
      safetySettings[i] = {
        category=cat,
        threshold=HarmBlockThreshold,
      }
    end
  end

  local ver = model:match"^gemini-(%d.&d)"
  local thinking = ver and (ver>="2.0" or ver=="2.0" and (model:match"thinking" or model:match"pro" or stream))
  --https://cloud.google.com/vertex-ai/generative-ai/docs/thinking#budget
  thinkingBudget=number(thinkingBudget) --0..24k
  local thinking_config = (thinkingBudget or thinking) and {
    includeThoughts=true,--?
    thinkingBudget=thinkingBudget,
  } or nil

  local contents = session and U.getHistory(historyName) or {}
  --https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#request
  local data = {
    contents=contents,
    safetySettings=safetySettings,
    systemInstruction=role and {
      parts={{text=role}},
    },
    --https://ai.google.dev/api/generate-content#generationconfig
    generationConfig={
      stopSequences=obj(stopSequences),
      responseMimeType=responseMimeType,
      --https://spec.openapis.org/oas/v3.0.3#schema
      responseSchema=obj(responseSchema), --https://ai.google.dev/gemini-api/docs/structured-output?lang=rest
      --responseModalities={TEXT,IMAGE,AUDIO},
      seed=number(seed),
      candidateCount=number(candidateCount),
      maxOutputTokens=number(maxOutputTokens),
      temperature=number(temperature),
      topP=number(top_p),
      topK=number(top_k),
      presencePenalty=number(presencePenalty), --number
      frequencyPenalty=number(frequencyPenalty), --number
      responseLogprobs=boolean(responseLogprobs),
      logprobs=number(logprobs), --integer
      --enableEnhancedCivicAnswers=true,
      --speechConfig={voiceConfig={voiceName= string }, languageCode="ru-RU"},
      --?audioTimestamp=true,
      thinking_config=thinking_config,
      --mediaResolution=MEDIA_RESOLUTION_LOW|MEDIA_RESOLUTION_MEDIUM|MEDIA_RESOLUTION_HIGH,
    },
    tools=obj(tools),
  }
  table.insert(contents, {role="user", parts={text=context}})

  return function (cb)
    local _parts = {}
    local chunks = {}
    local function ln() return chunks[1] and "\n\n" or "" end
    --https://ai.google.dev/api/generate-content#endpoint
    local response,meta = client:generate(data, model, stream, function (chunk,err,extra)
      if err then
        cb(ln()); chunks[1] = chunks[1] or ""
        if type(extra)=="string" then
          err = err.." "..extra
        elseif type(extra)=="table" then
          return cb(U.formatErrMsg({ statusline=err, data=extra }, "\n", getMsgText).."\n")
        end
        return cb("Error: "..err)
      end
      --https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#response
      local candidate = chunk.candidates[1]
      if not candidate then
        local reason = chunk.promptFeedback and chunk.promptFeedback.blockReason
        if reason then
          cb(ln().."promptFeedback.blockReason: "..reason..formatCategory(chunk.promptFeedback.safetyRatings))
        end
        return
      end
      assert(#chunk.candidates==1, "Only 1 candidate expected")
      assert(not candidate.index or candidate.index==0, "Unexpected candidate index")
      --https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference
      --.safetyRatings
      --.citationMetadata
      --.avgLogprobs
      --.logprobsResult.topCandidates[].candidates[]
      --.logprobsResult.choosenCandidates[].candidates[]
      --.usageMetadata
      --.modelVersion
      local delimiter = "\n\n|>"
      local parts = candidate.content and candidate.content.parts
      if parts then
        for i,part in ipairs(parts) do
          if not part.text or i>1 then
            if chunks[1] then
              table.insert(_parts, {text=table.concat(chunks)})
              chunks = {}
            end
          end
          if part.text then
            if i>1 then --gemini-2.0-flash-thinking-exp
              cb(delimiter)
            end
            cb(part.text, chunk.modelVersion~=model and chunk.modelVersion)
            table.insert(chunks, part.text)
          else
            cb("\n"..processNonText(part).."\n")
          end
        end
      else --debug
        cb("Error: no content\n"..U.formatJson(candidate))
        --??if not candidate.safetyRatings then return end
        --todo:
        --{"citationMetadata":{"citationSources":[{"uri":"https:\/\/forum.farmanager.com\/viewtopic.php?p=171909","startIndex":5254,"endIndex":5517}]},"finishReason":"RECITATION"}
      end
      local reason = candidate.finishReason
      if reason and reason~="STOP" and type(reason)=="string" then
        cb(ln().."finishReason: "..reason..formatCategory(candidate.safetyRatings))
      end
      if U.check"Esc" then error("interrupted", 0) end
    end)

    if response==U.restapi.STREAMED then
      table.insert(_parts, {text=table.concat(chunks)})
      table.insert(contents, {role="model", parts=_parts})
    elseif response then
      table.insert(contents, response.candidates[1].content)
      for i,candidate in ipairs(response.candidates) do
        local parts = candidate.content and candidate.content.parts
        if parts then
          for ii,part in ipairs(parts) do
            if part.text then
              if i>1 or ii>1 then cb("\n\n") end
              cb(part.text)
            else
              cb("\n"..processNonText(part).."\n")
            end
          end
        else --debug
          cb("Error: no content\n"..U.formatJson(candidate))
        end
        local reason = candidate.finishReason
        if reason and reason~="STOP" and type(reason)=="string" then
          cb("\nfinishReason: "..reason..formatCategory(candidate.safetyRatings))
        end
      end
      local reason = response.promptFeedback and response.promptFeedback.blockReason
      if reason then
        cb(ln().."promptFeedback.blockReason: "..reason..formatCategory(response.promptFeedback.safetyRatings))
      end
    else
      table.remove(contents)
      if not chunks[1] then
        cb(ln()..U.formatErrMsg(meta,"\n\n",getMsgText))
      end
    end
    cb()
  end
end,

--https://ai.google.dev/api/models#endpoint_1
function (data) -- getModels
  local client = U.restapi.GoogleGemini(data.apikey)
  client.formatErr = function (errMeta)
    return U.formatErrMsg(errMeta,"\n\1\n",getMsgText)
  end
  local models = assert(client:models())
  models.filterFn = function (model)
    for _,method in ipairs(model.supportedGenerationMethods) do
      if method=="generateContent" then
        return true
      end
    end
  end
  models.nameFn = function (model)
    return model.name:match"models/(.+)"
  end
  return models
end
