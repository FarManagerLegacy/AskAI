name = "Google Gemini"
url = "https://aistudio.google.com/"

local U,api = ...
if not U.restapi then return end
api = api or U.restapi.GoogleGemini

local function getMsgText (data)
  local msg = data.message or data.error or data
  msg = msg.error or msg
  return msg.message or msg
end
U.restapi.GoogleGemini.__base.formatErr = function (errMeta)
  return U.formatErrMsg(errMeta,"\n\1\n",getMsgText)
end

exe = true

historyName = name

predefined = {
  api_base={
    "https://generativelanguage.googleapis.com/v1beta",
    "https://generativelanguage.googleapis.com/v1alpha",
    "https://generativelanguage.googleapis.com/v1"
  },
  model="gemini-flash-latest",
  apikey=win.GetEnv"GEMINI_API_KEY",
  responseMimeType={
    "",
    "text/plain", -- (default)
    "application/json",
    "text/x.enum", --https://ai.google.dev/gemini-api/docs/file-prompting-strategies#plain_text_formats
  },
  HarmBlockThreshold={ --https://ai.google.dev/api/generate-content#HarmBlockThreshold
    "",
    "HARM_BLOCK_THRESHOLD_UNSPECIFIED",	--Threshold is unspecified.
    "BLOCK_LOW_AND_ABOVE",	--Content with NEGLIGIBLE will be allowed.
    "BLOCK_MEDIUM_AND_ABOVE",	--Content with NEGLIGIBLE and LOW will be allowed.
    "BLOCK_ONLY_HIGH",	--Content with NEGLIGIBLE, LOW, and MEDIUM will be allowed.
    "BLOCK_NONE",	--All content will be allowed.
    "OFF",	--Turn off the safety filter.
  },
  tools={
    "",
    "codeExecution:{}",--https://ai.google.dev/gemini-api/docs/code-execution?lang=rest
    "googleSearch:{}", --https://ai.google.dev/gemini-api/docs/google-search
    "urlContext:{}",   --https://ai.google.dev/gemini-api/docs/url-context
  },
  --Gemini 2.5: https://ai.google.dev/gemini-api/docs/thinking#levels-budgets
  --Gemini 3:   https://ai.google.dev/gemini-api/docs/thinking#thinking-levels
  thinkingLevel={
    "",
    "low",     --1024
    "high",    --24576 (default)
    -- Flash only;
    "minimal", --1024
    "medium",  --8192
  },
}

hidden = {
  "candidateCount",
  "responseMimeType", "responseSchema",
  "seed", "stopSequences",
  "top_p", "top_k", "presencePenalty", "frequencyPenalty", "responseLogprobs", "logprobs", "HarmBlockThreshold",
}

--https://ai.google.dev/api/generate-content#BlockReason
--https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters
local function formatCategory (safetyRatings)
  if not safetyRatings then return "" end
  local out = {}
  for _,rating in ipairs(safetyRatings) do
    if rating.blocked then
      out[#out+1] = ("%s: %s"):format(rating.category, rating.probability)
    end
  end
  if #out==0 then return "" end
  return (" (%s)"):format(table.concat(out, ", "))
end

local function wrapCode (data)
  if data.outcome and data.outcome~="OUTCOME_OK" then
    return data.outcome
  end
  return ("```%s\n%s\n```"):format(data.language or "output", data.code or data.output)
end

local function processNonText (part)
  local execData = part.executableCode or part.codeExecutionResult
  if execData then
    --https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/code-execution-api
    --https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/code-execution
    return wrapCode(execData)
  else --todo
    --https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/function-calling#rest-openai
    --https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#examples_of_function_calling
    return "Error: unknown part's type\n"..U.formatJson(part)
  end
end

local function redirect (uri)
  if not uri:find("/grounding-api-redirect/",1,"plain") then
    return uri
  end
  local _,code,headers = assert(require"ssl.https".request{url=uri,redirect=false})
  assert(code==302, "Unexpected response code")
  return headers.location:gsub('%%(%x%x)', function(hex) --urldecode
    return string.char(tonumber(hex, 16))
  end)--:gsub("+", " ")
end
local function processGrounding (chunks)
  if chunks then
    local out = {"\n"}
    for _,data in ipairs(chunks) do
      out[#out+1] = ("- [%s](%s)"):format(data.web.title, redirect(data.web.uri))
    end
    return table.concat(out,"\n")
  end
  return ""
end

--https://ai.google.dev/gemini-api/docs/safety-settings
local HarmCategory = { --https://ai.google.dev/api/generate-content#harmcategory
  --"HARM_CATEGORY_UNSPECIFIED",	--Category is unspecified.
  "HARM_CATEGORY_HARASSMENT",	--Harasment content.
  "HARM_CATEGORY_HATE_SPEECH",	--Hate speech and content.
  "HARM_CATEGORY_SEXUALLY_EXPLICIT",	--Sexually explicit content.
  "HARM_CATEGORY_DANGEROUS_CONTENT",	--Dangerous content.
  "HARM_CATEGORY_CIVIC_INTEGRITY",	--Content that may be used to harm civic integrity.
}
--https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages
return function (session, stream, context, api_base, apikey, model, temperature, role,
                 thinkingLevel, thinkingBudget, includeThoughts,
                 maxOutputTokens, candidateCount, responseMimeType, responseSchema, seed, stopSequences, tools,
                 top_p, top_k, presencePenalty, frequencyPenalty, responseLogprobs, logprobs, HarmBlockThreshold)
  local number,obj,boolean = U.number, U.obj, U.boolean
  local client = api(apikey, api_base)

  local safetySettings --https://ai.google.dev/api/generate-content#safetysetting
  if HarmBlockThreshold then
    safetySettings = {}
    for i,cat in ipairs(HarmCategory) do
      safetySettings[i] = {
        category=cat,
        threshold=HarmBlockThreshold,
      }
    end
  end

  assert(model, "model is required")
  local ver = model:match"-latest$" and "2.5" or model:match"^gemini%-(%d.%d)" or model:match"^gemini%-(%d)%-" or "3"
  local thinking = ver>"2.0" or ver=="2.0" and (model:match"thinking" or model:match"pro" or stream)
  --https://cloud.google.com/vertex-ai/generative-ai/docs/thinking#budget
  local thinkingConfig = thinking and {
    includeThoughts=boolean(includeThoughts),
    thinkingBudget=number(thinkingBudget),
    thinkingLevel=thinkingLevel,
  } or nil

  local contents = session and U.getHistory(historyName) or {}
  --https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#request
  local data = {
    contents=contents,
    safetySettings=safetySettings,
    systemInstruction=role and {
      parts={{text=role}},
    },
    --https://ai.google.dev/api/generate-content#generationconfig
    generationConfig={
      stopSequences=obj(stopSequences),
      responseMimeType=responseMimeType,
      --https://spec.openapis.org/oas/v3.0.3#schema
      responseSchema=obj(responseSchema), --https://ai.google.dev/gemini-api/docs/structured-output?lang=rest
      --_responseJsonSchema=obj(responseSchema),
      --responseModalities={TEXT,IMAGE,AUDIO},
      seed=number(seed),
      candidateCount=number(candidateCount),
      maxOutputTokens=number(maxOutputTokens),
      temperature=number(temperature),
      topP=number(top_p),
      topK=number(top_k),
      presencePenalty=number(presencePenalty),
      frequencyPenalty=number(frequencyPenalty),
      responseLogprobs=boolean(responseLogprobs),
      logprobs=number(logprobs), --integer
      --enableEnhancedCivicAnswers=true,
      --speechConfig={voiceConfig={voiceName= string }, languageCode="ru-RU"},
      --?audioTimestamp=true,
      --imageConfig={aspectRatio= string, imageSize= string},
      thinkingConfig=thinkingConfig,
      --mediaResolution=MEDIA_RESOLUTION_LOW|MEDIA_RESOLUTION_MEDIUM|MEDIA_RESOLUTION_HIGH,
    },
    tools=obj(tools),
  }
  table.insert(contents, {role="user", parts={text=context}})

  return function (cb)
    local _parts = {}
    local chunks = {}
    local function ln() return chunks[1] and "\n\n" or "" end
    --https://ai.google.dev/api/generate-content#endpoint
    local response,meta = client:generate(data, model, stream, function (chunk,err,extra)
      if err then
        cb(ln()); chunks[1] = chunks[1] or ""
        if type(extra)=="string" then
          err = err.." "..extra
        elseif type(extra)=="table" then
          return cb(U.formatErrMsg{ statusline=err, data=extra }.."\n")
        end
        return cb("Error: "..err)
      end
      --https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#response
      chunk = chunk.response or chunk -- Gemini CLI
      local candidate = chunk.candidates[1]
      if not candidate then
        local reason = chunk.promptFeedback and chunk.promptFeedback.blockReason
        if reason then
          cb(ln().."promptFeedback.blockReason: "..reason..formatCategory(chunk.promptFeedback.safetyRatings))
        end
        return
      end
      assert(#chunk.candidates==1, "Only 1 candidate expected")
      assert(not candidate.index or candidate.index==0, "Unexpected candidate index")
      --https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference
      --.safetyRatings
      --.citationMetadata
      --.avgLogprobs
      --.logprobsResult.topCandidates[].candidates[]
      --.logprobsResult.choosenCandidates[].candidates[]
      --.usageMetadata
      --.modelVersion
      local parts = candidate.content and candidate.content.parts
      local part = parts and parts[1]
      if part then
        assert(#parts==1, "Only 1 part expected")
        if not part.text then
          if chunks[1] then
            assert(not part.thoughtSignature, "Unexpected thoughtSignature")
            table.insert(_parts, {text=table.concat(chunks)})
            chunks = {}
          end
        end
        if part.text then
          if part.thought~=chunks.isThinking then
            cb(part.thought and ln().."<think>\n" or "</think>\n\n\n")
            if chunks[1] then
              if not chunks.isThinking then
                table.insert(_parts, {text=table.concat(chunks), thought=chunks.isThinking})
              end
              if chunks.thoughtSignature then
                table.insert(_parts, {text="", thoughtSignature=chunks.thoughtSignature})
              end
              chunks = {}
            end
            chunks.isThinking = part.thought
          end
          local m = chunk.modelVersion:match"^models/(.+)"
          cb(part.text, chunk.modelVersion~=model and m~=model and m)
          table.insert(chunks, part.text)
          if part.thoughtSignature then
            assert(not chunks.thoughtSignature, "Unexpected multiple thoughtSignature")
            chunks.thoughtSignature = part.thoughtSignature
          end
        else
          cb("\n"..processNonText(part).."\n")
        end
      else --debug
        cb("Error: no content\n"..U.formatJson(candidate))
        --??if not candidate.safetyRatings then return end
      end
      if candidate.citationMetadata then --debug
        if next(candidate.citationMetadata) then
          --cb("\ncitationMetadata:\n"..U.formatJson(candidate.citationMetadata).."\n")
        end
      end
      if candidate.groundingMetadata then
        cb(processGrounding(candidate.groundingMetadata.groundingChunks),nil,"nowrap")
      end
      local reason = candidate.finishReason
      if reason and reason~=U.json.null and reason~="STOP" then
        cb(ln().."finishReason: "..reason..formatCategory(candidate.safetyRatings))
        if candidate.finishMessage then
          cb("\nfinishMessage: "..candidate.finishMessage)
        end
      end
      if U.utils.check"Esc" then error("interrupted", 0) end
    end)

    if response==U.restapi.STREAMED then
      table.insert(_parts, {text=table.concat(chunks)})
      if chunks.thoughtSignature then
        table.insert(_parts, {text="", thoughtSignature=chunks.thoughtSignature})
      end
      table.insert(contents, {role="model", parts=_parts})
    elseif response then
      response = response.response or response -- Gemini CLI
      table.insert(contents, response.candidates[1].content)
      for i,candidate in ipairs(response.candidates) do
        local parts = candidate.content and candidate.content.parts
        if parts then
          for ii,part in ipairs(parts) do
            if part.text then
              if i>1 or ii>1 then cb("\n\n") end
              if part.thought then
                cb("<think>\n")
                cb(part.text:match"^(.-)%s*$") -- trim new lines at end
                cb("\n</think>")
              else
                cb(part.text)
              end
            else
              cb("\n"..processNonText(part).."\n")
            end
          end
        else --debug
          cb("Error: no content\n"..U.formatJson(candidate))
        end
        if candidate.groundingMetadata then
          cb(processGrounding(candidate.groundingMetadata.groundingChunks),nil,"nowrap")
        end
        local reason = candidate.finishReason
        if reason and reason~=U.json.null and reason~="STOP" then
          cb("\nfinishReason: "..reason..formatCategory(candidate.safetyRatings))
          if candidate.finishMessage then
            cb("\nfinishMessage: "..candidate.finishMessage)
          end
        end
      end
      local reason = response.promptFeedback and response.promptFeedback.blockReason
      if reason then
        cb(ln().."promptFeedback.blockReason: "..reason..formatCategory(response.promptFeedback.safetyRatings))
      end
    else
      table.remove(contents)
      if not chunks[1] then
        cb(ln()..U.formatErrMsg(meta,"\n\n",getMsgText))
      end
    end
    cb()
  end
end,

--https://ai.google.dev/api/models#endpoint_1
function (data) -- getModels
  local client = U.restapi.GoogleGemini(data.apikey, data.api_base)
  local models = assert(client:models())
  models.filterFn = function (model)
    for _,method in ipairs(model.supportedGenerationMethods) do
      if method=="generateContent" then
        return true
      end
    end
  end
  models.nameFn = function (model)
    return model.name:match"models/(.+)"
  end
  return models
end
