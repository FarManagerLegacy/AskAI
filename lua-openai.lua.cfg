url = "https://github.com/leafo/lua-openai/releases"
name = "lua-openai"
exe = pcall(require, "openai")

predefined = {
  apibase={
    "https://api.openai.com/v1",
    "https://my-openai-gemini-demo.vercel.app/v1",
    "https://copilot-demo.publicaffairs.workers.dev/v1"
  },
  --https://platform.openai.com/docs/models/overview
  model={"gpt-3.5-turbo", "gpt-4", "gpt-4o", "gpt-4o-mini", "gpt-4-turbo", "gpt-4-turbo-preview"},
  response_format={'{ type="json_object" }'},
  role={"You are a friendly AI assistant"},
}

local State = ...

clearSession = function ()
  State.luaOpenAiMessages = nil
end
showSession = function () --todo
  require"le"(State.luaOpenAiMessages)
end

local function boolean (str, default)
  if type(str)=="number" then
    return str==1
  elseif type(str)=="boolean" then
    return str
  elseif str=="null" then
    return nil
  elseif str=="false" then
    return false
  else
    return str=="true" or default
  end
end
local function number (str)
  return str and tonumber(str) or str
end
local function obj (str)
  if type(str)=="string" then
    local fn = assert(loadstring("return "..str))
    str = setfenv(fn,{})()
  end
  return str
end

local function formatErrMsg (response,status)
  if type(response)=="string" then
    local msg = response
    status = tostring(status)
    if msg:sub(1, status:len())==status then
      msg = msg:sub(status:len()+2)
    end
    if not msg:lower():find("error") then
      msg = ("Error %s: %s"):format(status,msg)
    end
    return msg
  else -- table
    local msg = response.message or response.error or response
    msg = msg and msg.error or msg
    msg = msg and msg.message or msg
    if type(msg)=="string" then
      if not msg:lower():find("error") then
        msg = ("Error %s: %s"):format(status,msg)
      end
      return msg
    else
      return ("Error %s: %s"):format(status, require("cjson").encode(response))
    end
  end
end

--https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages
return function (session, stream, context, apibase, apikey, model, temperature, role, frequency_penalty, logit_bias,
                 logprobs, top_logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, top_p, headers)
  local openai = require("openai")
  local client = openai.new(apikey)
  if apibase then
    client.api_base = apibase
    client.config = {
      http_provider = apibase:match"https://" and "ssl.https"
                   or apibase:match"http://" and "socket.http"
                   or error("unknown protocol: "..apibase)
    }
  end
  if headers then
    client.headers = obj(headers)
  end
  local opts = {
    model=model,
    frequency_penalty=number(frequency_penalty),
    logit_bias=obj(logit_bias),
    logprobs=boolean(logprobs),
    top_logprobs=number(top_logprobs),
    max_tokens=number(max_tokens),
    n=number(n),
    presence_penalty=number(presence_penalty),
    response_format=obj(response_format),
    seed=number(seed),
    stop=stop,
    stream = stream,
    temperature=number(temperature),
    top_p=number(top_p),
  }
  local messages = {}
  if session then
    State.luaOpenAiMessages = State.luaOpenAiMessages or {}
    messages = State.luaOpenAiMessages
  end
  if role then
    if messages[1] and messages[1].role=="system" then
      messages[1].content = role
    else
      table.insert(messages, 1, {role = "system", content = role})
    end
  end
  table.insert(messages, {role = "user", content = context})
  return function (cb)
    local lastIdx, streamed = 0
    local null = require"cjson".null
    local chunks = {}
    local status, response = client:chat(messages, opts, function (chunk)
      local candidate = chunk.choices[1]
      if not candidate then return end
      local reason = candidate.finish_reason
      if candidate.index~=lastIdx then
        cb("\n\n")
        lastIdx = candidate.index
      end
      if candidate.delta.content then
        if candidate.index==0 then
          table.insert(chunks, candidate.delta.content)
        end
        cb(candidate.delta.content, chunk.model~=model and chunk.model)
      end
      if reason and reason~="stop" and reason~=null then
        cb("\nfinish_reason: "..reason)
      end
      streamed = true
    end)
    if status==200 then
      if streamed then
        table.insert(messages, {role = "assistant", content = table.concat(chunks)})
      else
        if type(response)=="table" then
          table.insert(messages, {role = "assistant", content = response.choices[1].message.content})
          for i,candidate in ipairs(response.choices) do
            if i>1 then
              cb("\n\n")
            end
            cb(candidate.message.content)
            local reason = candidate.finish_reason
            if reason and reason~="stop" and reason~=null then
              cb("\nfinish_reason: "..reason)
            end
          end
        else --debug
          far.Message("Unexpected response:\n"..tostring(response), name, nil, "w")
          if type(response)=="string" then
            far.CopyToClipboard(response)
          end
        end
      end
    else -- status~=200
      cb(formatErrMsg(response,status))
    end
    cb()
  end
end
