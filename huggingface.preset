--https://huggingface.co/docs/api-inference/index

apibase="https://api-inference.huggingface.co/models/{model}/v1"
apikey=win.GetEnv"HF_API_TOKEN" or "set HF_API_TOKEN env variable!"

-- https://huggingface.co/models?other=text-generation-inference&inference=warm
-- https://huggingface.co/models?other=text-generation-inference&inference=cold
model={
  "meta-llama/Llama-3.2-11B-Vision-Instruct",
  "google/gemma-2-27b-it",
  "HuggingFaceH4/starchat2-15b-v0.1",
  "microsoft/Phi-3-mini-4k-instruct",
  "mistralai/Mistral-Small-Instruct-2409",
  "mistralai/Mistral-Nemo-Instruct-2407",
  "mistralai/Mistral-7B-v0.1",
  "mistralai/Mistral-7B-Instruct-v0.1",
  "mistralai/Mistral-7B-Instruct-v0.2",
  "mistralai/Mistral-7B-Instruct-v0.3",
  "mistralai/Mixtral-8x7B-Instruct-v0.1",
  "google/gemma-2-2b-it",
  "NousResearch/Hermes-3-Llama-3.1-8B",
  "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
}
modelsFn = "none"

local useCache = '"x-use-cache":"false"'
headers = function()
  local ans = far.Message("Wait for model?","HuggingFace preset","No;Yes")
  local waitModel = ans==2 and '"x-wait-for-model":"true"' or nil
  return table.concat({useCache,waitModel},",")
end
