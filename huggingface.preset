--https://huggingface.co/docs/api-inference/index

apibase="https://api-inference.huggingface.co/models/{model}/v1"
apikey=win.GetEnv"HF_API_TOKEN" or "set HF_API_TOKEN env variable!"

-- https://huggingface.co/models?other=text-generation-inference&inference=warm
-- https://huggingface.co/models?other=text-generation-inference&inference=cold
model={
  "codellama/CodeLlama-34b-Instruct-hf",
  "google/gemma-1.1-2b-it",
  "google/gemma-1.1-7b-it",
  "google/gemma-2-2b-it",
  "google/gemma-2-9b-it",
  "google/gemma-2b",
  "HuggingFaceH4/starchat2-15b-v0.1",
  "HuggingFaceH4/zephyr-7b-beta",
  "meta-llama/Llama-2-7b-chat-hf",
  --"meta-llama/Llama-3.1-70B-Instruct",
  "meta-llama/Llama-3.1-8B-Instruct",
  "meta-llama/Llama-3.2-1B-Instruct",
  "meta-llama/Llama-3.2-3B-Instruct",
  --"meta-llama/Meta-Llama-3-70B-Instruct",
  "meta-llama/Meta-Llama-3-8B-Instruct",
  "microsoft/Phi-3-mini-4k-instruct",
  "microsoft/Phi-3.5-mini-instruct",
  "mistralai/Mistral-7B-Instruct-v0.1",
  "mistralai/Mistral-7B-Instruct-v0.2",
  "mistralai/Mistral-7B-Instruct-v0.3",
  "mistralai/Mistral-Nemo-Instruct-2407",
  "mistralai/Mixtral-8x7B-Instruct-v0.1",
  "NousResearch/Hermes-3-Llama-3.1-8B",
  "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
  "Qwen/Qwen2.5-72B-Instruct",
  "tiiuae/falcon-7b-instruct",
}
modelsMeta = "none"

local useCache = '"x-use-cache":"false"'
headers = function()
  local ans = far.Message("Wait for model?","HuggingFace preset","No;Yes")
  local waitModel = ans==2 and '"x-wait-for-model":"true"' or nil
  return table.concat({useCache,waitModel},",")
end
