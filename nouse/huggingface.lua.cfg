--https://huggingface.co/docs/api-inference/index
name = "huggingface.co api inference"
local applyParams = _import "openai.lua.cfg"
if not exe then return end

predefined = {
  -- https://huggingface.co/models?other=text-generation-inference&inference=warm&
  -- https://huggingface.co/models?other=text-generation-inference&inference=cold&
  model={
    "meta-llama/Llama-3.2-11B-Vision-Instruct",
    "mistralai/Mixtral-8x7B-Instruct-v0.2",
    "mistralai/Mistral-Nemo-Instruct-2407",
    "google/gemma-2-2b-it",
    "microsoft/Phi-3-mini-4k-instruct",
    "HuggingFaceH4/starchat2-15b-v0.1",
  },
  token=win.GetEnv"HF_API_TOKEN",
}

hidden = nil

return function (session, stream, context, token, model, temperature, role, max_tokens, waitformodel)
  local apikey = assert(token, "token required"):match"^hf_.+"
  local cookie = not apikey and "token="..token or nil
  return applyParams {
    session, stream, context,
    apibase=("https://api-inference.huggingface.co/models/%s/v1"):format(assert(model, "model not specified")),
    apikey=apikey, 
    model=model,
    temperature=temperature,
    role=role,
    max_tokens=max_tokens,
    headers={
      ["x-use-cache"]="false",
      ["x-wait-for-model"]=waitformodel and "true" or nil,
      cookie=cookie,
    },
  }
end
