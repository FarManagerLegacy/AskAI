name = "OpenAI-compatible"
url = "https://platform.openai.com/playground"

local U = ...
if not U.restapi then return end
exe = true

historyName = name

predefined = {
  --https://platform.openai.com/docs/models/overview
  apibase="https://api.openai.com/v1",
  model={
    "gpt-5.2", "gpt-5.2-chat-latest", "gpt-5.2-pro",
    "gpt-5.1", "gpt-5.1-chat-latest",
    "gpt-5",   "gpt-5-chat-latest", "gpt-5-mini", "gpt-5-nano", "gpt-5-pro",
    "gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano",
    "gpt-4o", "gpt-4o-mini",
    "o1", "o3", "o3-mini", "o4-mini",
  },
  --https://platform.openai.com/docs/api-reference/chat/create#chat_create-reasoning_effort
  reasoning_effort={"", "none", "minimal", "low", "medium", "high", "xhigh"},
  response_format={"", '{ type="json_object" }'},
  role={"", "You are a friendly AI assistant"},
  verbosity={"", "low", "medium", "high"},
}

sets = {
  apibase = {"apikey", "model", "extra_body", "headers", "modelsMeta"}
}

hidden = { -- to use in presets
  "headers",
  "max_completion_tokens",
  "repetition_penalty",
  "top_k",
  "modelsMeta",
}

local function importArgs (args)
  local level = 2
  for i=1,debug.getinfo(level, "u").nparams do
    local name = debug.getlocal(level, i)
    debug.setlocal(level, i, args[name] or args[i])
  end
end

local function getMsgText (data)
  local msg = data.message or data.detail or data.error or data
  msg = msg.message or msg -- OpenRouter error
  msg = msg.error or msg   --
  return msg.message or msg
end

local function getModels (data)
  local meta
  if data.modelsMeta and data.modelsMeta~="none" then
    meta = U.obj(data.modelsMeta)
    if type(meta)~="table" then
      error "modelsMeta must be table or 'none'"
    end
  else
    meta = {}
  end
  local models
  if meta.modelsFn then
    models = setfenv(meta.modelsFn, setmetatable({json=U.json, api=U.restapi},{__index=_G}))(data)
  else
    if meta.dataFn then meta.dataFn(data) end
    local client = U.restapi.OpenAI(data.apikey, meta.apibase or data.apibase, U.obj(meta.headers or data.headers))
    if meta.modelsData then client.modelsData = meta.modelsData end
    client.formatErr = function (errMeta)
      return U.formatErrMsg(errMeta,"\n\1\n",getMsgText)
    end
    models = assert(client:models(meta.endpoint))
    models.keyId = meta.modelsData and meta.modelsData.id
  end
  models.filterFn = meta.filterFn
  models.nameFn = meta.nameFn
  return models
end

--https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages
return function (session, stream, context, apibase, apikey, model, temperature, role,
                logit_bias, logprobs, top_logprobs, max_tokens, max_completion_tokens, n,
                frequency_penalty, presence_penalty, repetition_penalty,
                reasoning_effort, response_format, seed, stop, top_k, top_p, verbosity,
                extra_body, headers, modelsMeta)
  local args = type(session)=="table" and session
  if args then importArgs(args) end
  local boolean,number,obj = U.boolean, U.number, U.obj
  if apibase then
    apibase = apibase:gsub("{model}", function() return assert(model,"model required") end)
                     :gsub("{model%-slug}", function() return assert(model,"model required"):gsub("[/.]","-"):lower() end)
  end
  local client = U.restapi.OpenAI(apikey, apibase, obj(headers))

  local messages = session and U.getHistory(historyName) or {}
  local data = {
    messages=messages,
    model=model,
    --audio=...
    frequency_penalty=number(frequency_penalty),
    logit_bias=obj(logit_bias),
    logprobs=boolean(logprobs),
    top_logprobs=number(top_logprobs),
    max_tokens=number(max_tokens), --deprecated
    max_completion_tokens=number(max_completion_tokens),
    --modalities={"text","audio"},
    n=number(n),
    presence_penalty=number(presence_penalty),
    reasoning_effort=reasoning_effort,
    repetition_penalty=number(repetition_penalty), --Range: [0, 2] Not available for OpenAI models
    response_format=obj(response_format),
    seed=number(seed),
    --service_tier=
    stop=stop,
    stream=boolean(stream),
    --stream_options={include_usage=true,},
    temperature=number(temperature),
    top_k=number(top_k), --Range: [1, Infinity] Not available for OpenAI models
    top_p=number(top_p),
    verbosity=verbosity,
  }
  if extra_body then
    for k,v in pairs(obj(extra_body)) do
      data[k] = v
    end
  end
  if role then
    if messages[1] and messages[1].role=="system" then
      messages[1].content = role
    else
      table.insert(messages, 1, {role="system", content=role})
    end
  end
  table.insert(messages, {role="user", content=context})
  if args and args.hook then
    data = args.hook(client,data) or data
  end
  local CONTROL = 17 --CtrlEnter
  if win.GetKeyState and win.GetKeyState(CONTROL) then -- use completions endpoint
    data.messages = nil
    data.echo = true
    data.prompt = context
    local prompt,suffix = context:match"^(.-)>_<(.-)$"
    if suffix then
      data.prompt, data.suffix = prompt, suffix
    end
    --data.best_of = 2
    client.defaultEndpoint = "/completions"
    if apibase:find"mistral.ai/v1$" then
      client.defaultEndpoint = "/fim/completions"
    end
  end

  return function (cb)
    local lastIdx = 0
    local chunks, reasoning_chunks = {}, {}
    local isThinking, reasoning_key
    local function ln() return (chunks[1] or isThinking) and "\n\n" or "" end
    local function notNull (v)
      return v and v~=U.json.null and v or nil
    end
    local function isInteresting (reason)
      return notNull(reason) and reason~="stop" and reason~="eos" and reason~="" -- together.ai: eos
    end
    local response,meta = client:generate(data, function (chunk,err,extra)
      if err then
        cb(ln()); chunks[1] = chunks[1] or ""
        if type(extra)=="string" then
          err = err.." "..extra
        elseif type(extra)=="table" then
          return cb(U.formatErrMsg({ statusline=err, data=extra }, "\n", getMsgText).."\n")
        end
        return cb("Error: "..err)
      end
      if chunk.error then --OpenRouter
        --le(chunk, "ERROR")
        chunks[1] = chunks[1] or ""
        return cb(ln()..("Error %s: %s"):format(chunk.error.code, chunk.error.message))
      end
      local candidate = chunk.choices[1]
      if not candidate then return end
      candidate.index = candidate.index or 0
      if candidate.index~=lastIdx then
        cb("\n\n")
        lastIdx = candidate.index
      end
      local message = candidate.delta
      if candidate.text then
        if candidate.index==0 then
          table.insert(chunks, candidate.text)
        end
        cb(candidate.text, chunk.model~=model and notNull(chunk.model))
      elseif notNull(message) then
        local reasoning = message.reasoning or message.reasoning_content
        --OpenRouter: additionally `message.reasoning_details`
        --https://openrouter.ai/docs/guides/best-practices/reasoning-tokens#reasoning_details-array-structure
        if notNull(reasoning) and reasoning~="" then
          reasoning_key = reasoning_key or message.reasoning and "reasoning" or "reasoning_content"
          if not isThinking then
            isThinking = true
            cb("<think>\n")
            reasoning = reasoning:match"^\n?(.*)"
          end
          if candidate.index==0 then
            table.insert(reasoning_chunks, reasoning)
          end
          cb(reasoning, chunk.model~=model and notNull(chunk.model))
        elseif notNull(message.refusal) then
          cb("refusal: "..message.refusal)
        elseif notNull(message.content) then
          if isThinking then
            isThinking = false
            cb("\n</think>\n\n")
          end
          if candidate.index==0 then
            table.insert(chunks, message.content)
          end
          cb(message.content, chunk.model~=model and notNull(chunk.model))
        --else le(candidate.delta)
        end
      end
      if isInteresting(candidate.finish_reason) then
        cb(ln().."finish_reason: "..candidate.finish_reason)
      end
      if U.utils.check"Esc" then error("interrupted", 0) end
      -- Gemini ----------------
      if candidate.index==0 then
        local google = message.extra_content and message.extra_content.google
        if google then
          chunks.thought = chunks.thought or google.thought
          chunks.thought_signature = chunks.thought_signature or google.thought_signature
        end
      end
    end)

    if response==U.restapi.STREAMED then
      local message = {role="assistant", content=table.concat(chunks)}
      if reasoning_key then message[reasoning_key] = table.concat(reasoning_chunks) end
      if chunks.thought or chunks.thought_signature then -- Gemini
        message.extra_content = {
          google={
            thought=chunks.thought,
            thought_signature=chunks.thought_signature,
          }
        }
      end
      table.insert(messages, message)
    elseif response then
      local cand1 = response.choices[1]
      local message = cand1.message
      message.content = cand1.text or message.content
      table.insert(messages, message)
      for i,candidate in ipairs(response.choices) do
        if i>1 then cb("\n\n") end
        local reasoning = candidate.message.reasoning or candidate.message.reasoning_content
        if notNull(reasoning) then
          cb("<think>\n" .. reasoning:match"^\n?(.-)\n*$" .. "\n</think>\n\n")
        end
        local content = candidate.text or candidate.message.content
        if notNull(content) then
          cb(content)
        end
        local refusal = candidate.message.refusal
        if notNull(refusal) then
          cb("refusal: "..refusal)
        end
        if isInteresting(candidate.finish_reason) then
          cb("\nfinish_reason: "..candidate.finish_reason)
        end
      end
    else
      table.remove(messages)
      if not chunks[1] then
        cb(ln()..U.formatErrMsg(meta,"\n\n",getMsgText))
      end
    end
    cb()
  end
end,

getModels
