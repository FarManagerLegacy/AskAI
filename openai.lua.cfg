name = "OpenAI-compatible"
url = "https://platform.openai.com/playground"

local U = ...
if not U.restapi then return end
exe = true

historyName = name

predefined = {
  --https://platform.openai.com/docs/models/overview
  apibase="https://api.openai.com/v1",
  model={"gpt-3.5-turbo", "gpt-4", "gpt-4o", "gpt-4o-mini", "o1-mini", "o1-preview", "o3-mini"},
  response_format={"", '{ type="json_object" }'},
  role={"", "You are a friendly AI assistant"},
}

sets = {
  apibase = {"apikey", "model", "headers", "modelsMeta"}
}

hidden = { -- to use in presets
  "headers",
  "max_completion_tokens",
  "repetition_penalty",
  "top_k",
  "modelsMeta",
}

local function importArgs (args)
  local level = 2
  for i=1,debug.getinfo(level, "u").nparams do
    local name = debug.getlocal(level, i)
    debug.setlocal(level, i, args[name] or args[i])
  end
end

local function getMsgText (data)
  local msg = data.message or data.detail or data.error or data
  msg = msg.message or msg -- Openrouter error
  msg = msg.error or msg   --
  return msg.message or msg
end

local function getModels (data, meta)
  if data.modelsMeta and data.modelsMeta~="none" then
    meta = U.obj(data.modelsMeta)
    if type(meta)~="table" then
      error "modelsMeta must be table or 'none'"
    end
  else
    meta = meta or {}
  end
  local models
  if meta.modelsFn then
    models = setfenv(meta.modelsFn, setmetatable({},{__index=_G}))()
  else
    if meta.dataFn then meta.dataFn(data) end
    local client = U.restapi.OpenAI(data.apikey, meta.apibase or data.apibase, U.obj(meta.headers or data.headers))
    if meta.modelsKeys then client.modelsKeys = meta.modelsKeys end
    client.formatErr = function (errMeta)
      return U.formatErrMsg(errMeta,"\n\1\n",getMsgText)
    end
    models = assert(client:models(meta.endpoint))
  end
  models.filterFn = meta.filterFn
  models.nameFn = meta.nameFn
  return models
end

--https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages
return function (session, stream, context, apibase, apikey, model, temperature, role,
                logit_bias, logprobs, top_logprobs, max_tokens, max_completion_tokens, n,
                frequency_penalty, presence_penalty, repetition_penalty,
                response_format, seed, stop, top_k, top_p,
                headers, modelsMeta)
  local args = type(session)=="table" and session
  if args then importArgs(args) end
  local boolean,number,obj = U.boolean, U.number, U.obj
  if apibase then
    apibase = apibase:gsub("{model}", function() return assert(model,"model required") end)
                     :gsub("{model%-slug}", function() return assert(model,"model required"):gsub("[/.]","-"):lower() end)
  end
  local client = U.restapi.OpenAI(apikey, apibase, obj(headers))

  local messages = session and U.getHistory(historyName) or {}
  local data = {
    messages=messages,
    model=model,
    --audio=...
    frequency_penalty=number(frequency_penalty),
    logit_bias=obj(logit_bias),
    logprobs=boolean(logprobs),
    top_logprobs=number(top_logprobs),
    max_tokens=number(max_tokens), --deprecated
    max_completion_tokens=number(max_completion_tokens),
    --modalities={"text","audio"},
    n=number(n),
    presence_penalty=number(presence_penalty),
    --reasoning_effort="low", --"medium" --"high"
    repetition_penalty=number(repetition_penalty), --Range: [0, 2] Not available for OpenAI models
    response_format=obj(response_format),
    seed=number(seed),
    --service_tier=
    stop=stop,
    stream=boolean(stream),
    --stream_options=
    temperature=number(temperature),
    top_k=number(top_k), --Range: [1, Infinity] Not available for OpenAI models
    top_p=number(top_p),

    --prompt?: string;
    --min_p?: number; // Range: [0, 1]
    --top_a?: number; // Range: [0, 1]

    -- OpenRouter-only: https://openrouter.ai/docs/api-reference/overview#completions-request-format
    --transforms?: string[]; --See "Prompt Transforms" section: openrouter.ai/docs/transforms
    --models?: string[]; --See "Model Routing" section: openrouter.ai/docs/model-routing
    --route?: 'fallback';
    --provider?: ProviderPreferences; --See "Provider Routing" section: openrouter.ai/docs/provider-routing
  }
  if role then
    if messages[1] and messages[1].role=="system" then
      messages[1].content = role
    else
      table.insert(messages, 1, {role="system", content=role})
    end
  end
  table.insert(messages, {role="user", content=context})
  if args and args.hook then
    data = args.hook(client,data) or data
  end
  local CONTROL = 17 --CtrlEnter
  if win.GetKeyState and win.GetKeyState(CONTROL) then -- use completions endpoint
    data.messages = nil
    data.echo = true
    data.prompt = context
    local prompt,suffix = context:match"^(.-)>_<(.-)$"
    if suffix then
      data.prompt, data.suffix = prompt, suffix
    end
    --data.best_of = 2
    client.defaultEndpoint = "/completions"
    if apibase:find"mistral.ai/v1$" then
      client.defaultEndpoint = "/fim/completions"
    end
  end

  return function (cb)
    local lastIdx = 0
    local chunks = {}
    local function ln() return chunks[1] and "\n\n" or "" end
    local function isInteresting (reason)
      return reason and reason~="stop" and reason~="eos" and reason~="" and type(reason)=="string" -- together.ai: eos
    end
    local thinking
    local response,meta = client:generate(data, function (chunk,err,extra)
      if err then
        cb(ln()); chunks[1] = chunks[1] or ""
        if type(extra)=="string" then
          err = err.." "..extra
        elseif type(extra)=="table" then
          return cb(U.formatErrMsg({ statusline=err, data=extra }, "\n", getMsgText).."\n")
        end
        return cb("Error: "..err)
      end
      local candidate = chunk.choices[1]
      if not candidate then return end
      candidate.index = candidate.index or 0
      if candidate.index~=lastIdx then
        cb("\n\n")
        lastIdx = candidate.index
      end
      local message = candidate.delta
      if candidate.text then
        if candidate.index==0 then
          table.insert(chunks, candidate.text)
        end
        cb(candidate.text, chunk.model~=model and chunk.model)
      elseif message and type(message)=="table" then
        local reasoning = message.reasoning or message.reasoning_content
        if reasoning and type(reasoning)=="string" then
          if not thinking then
            thinking = true
            cb("<think>\n")
          end
          cb(reasoning)
        elseif message.refusal and type(message.refusal)=="string" then
          cb("refusal: "..message.refusal)
        elseif message.content and type(message.content)=="string" then
          if thinking then
            thinking = false
            cb("</think>\n\n")
          end
          if candidate.index==0 then
            table.insert(chunks, message.content)
          end
          cb(message.content, chunk.model~=model and chunk.model)
        --else le(candidate.delta)
        end
      end
      if isInteresting(candidate.finish_reason) then
        cb(ln().."finish_reason: "..candidate.finish_reason)
      end
      if U.check"Esc" then error("interrupted", 0) end
    end)

    if response==U.restapi.STREAMED then
      table.insert(messages, {role="assistant", content=table.concat(chunks)})
    elseif response then
      local cand1 = response.choices[1]
      table.insert(messages, {role="assistant", content=cand1.text or cand1.message.content})
      for i,candidate in ipairs(response.choices) do
        if i>1 then cb("\n\n") end
        local reasoning = candidate.message.reasoning or candidate.message.reasoning_content
        if reasoning and type(reasoning)=="string" then
          cb("<think>\n"..reasoning.."</think>\n\n")
        end
        local content = candidate.text or candidate.message.content
        if content and type(content)=="string" then
          cb(content)
        end
        local refusal = candidate.message.refusal
        if refusal and type(refusal)=="string" then
          cb("refusal: "..refusal)
        end
        if isInteresting(candidate.finish_reason) then
          cb("\nfinish_reason: "..candidate.finish_reason)
        end
      end
    else
      table.remove(messages)
      if not chunks[1] then
        cb(ln()..U.formatErrMsg(meta,"\n\n",getMsgText))
      end
    end
    cb()
  end
end,

getModels
