name = "OpenAI-compatible"
url = "https://platform.openai.com/playground"

local U = ...
if not U.restapi then return end
exe = true

historyName = name

predefined = {
  --https://platform.openai.com/docs/models/overview
  apibase="https://api.openai.com/v1",
  model={
    "gpt-5.2", "gpt-5.2-chat-latest", "gpt-5.2-pro",
    "gpt-5.1", "gpt-5.1-chat-latest",
    "gpt-5",   "gpt-5-chat-latest", "gpt-5-mini", "gpt-5-nano", "gpt-5-pro",
    "gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano",
    "gpt-4o", "gpt-4o-mini",
    "o1", "o3", "o3-mini", "o4-mini",
  },
  --https://platform.openai.com/docs/api-reference/chat/create#chat_create-reasoning_effort
  reasoning_effort={"", "none", "minimal", "low", "medium", "high", "xhigh"},
  response_format={"", '{ type="json_object" }'},
  role={"", "You are a friendly AI assistant"},
  verbosity={"", "low", "medium", "high"},
}

sets = {
  apibase = {"apikey", "model", "extra_body", "headers",}
}
serviceFields = {"modelsMeta", "apidef",}
local function merge (dest,source) for _,v in ipairs(source) do table.insert(dest,v) end end
merge(sets.apibase, serviceFields)

hidden = { -- to use in presets
  "headers",
  "max_completion_tokens",
  "repetition_penalty", "top_k",
  "frequency_penalty", "presence_penalty", "top_p",
  "logit_bias", "logprobs", "top_logprobs",
  "n", "seed", "stop",
  "response_format",
}

local function importArgs (args)
  local level = 2
  for i=1,debug.getinfo(level, "u").nparams do
    local name = debug.getlocal(level, i)
    debug.setlocal(level, i, args[name] or args[i])
  end
end

local function getMsgText (data)
  local msg = data.message or data.detail or data.error or data
  msg = msg.message or msg -- OpenRouter error
  msg = msg.error or msg   --
  return msg.message or msg
end
U.restapi.OpenAI.__base.formatErr = function (errMeta)
  return U.formatErrMsg(errMeta,"\n\1\n",getMsgText)
end

local function customClass (apidef)
  if not apidef then return U.restapi.OpenAI end
  local name = apidef:match"^%s-class (%w+) extends OpenAI"
  local api = not U.O.debug and name and U.restapi.OpenAI[name]
  if not api then
    local _env = setmetatable({json=U.json}, {__index=function(_,k) return U.restapi[k] or _G[k] end})
    local _source = "custom api class"
    if name then
      _source = _source .. ": " .. name
    end
    local fn = assert(require"moonscript".loadstring(apidef,_source,"t",_env,nil))
    api = fn()
    if name then
      U.restapi.OpenAI[name] = api
    end
  end
  local mt = getmetatable(assert(api, "apidef must return value"))
  return assert(mt and mt.__call, "apidef must return class object") and api
end

local function getModels (data)
  local meta
  if data.modelsMeta and data.modelsMeta~="none" then
    meta = U.obj(data.modelsMeta)
    if type(meta)~="table" then
      error "modelsMeta must be table or 'none'"
    end
  else
    meta = {}
  end
  local models
  if meta.modelsFn then
    models = setfenv(meta.modelsFn, setmetatable({json=U.json, api=U.restapi},{__index=_G}))(data)
  else
    if meta.dataFn then meta.dataFn(data) end
    local api = customClass(data.apidef)
    local client = api(data.apikey, meta.apibase or data.apibase, U.obj(meta.headers or data.headers))
    if meta.modelsData then client.modelsData = meta.modelsData end
    models = assert(client:models(meta.endpoint))
    models.keyId = meta.modelsData and meta.modelsData.id
  end
  models.filterFn = meta.filterFn
  models.nameFn = meta.nameFn
  return models
end

--https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages
return function (session, stream, context, apibase, apikey, model, temperature, role,
                logit_bias, logprobs, top_logprobs, max_tokens, max_completion_tokens, n,
                frequency_penalty, presence_penalty, repetition_penalty,
                reasoning_effort, response_format, seed, stop, top_k, top_p, verbosity,
                extra_body, headers, modelsMeta, apidef)
  local args = type(session)=="table" and session
  if args then importArgs(args) end
  local boolean,number,obj = U.boolean, U.number, U.obj
  if apibase then
    apibase = apibase:gsub("{model}", function() return assert(model,"model required") end)
                     :gsub("{model%-slug}", function() return assert(model,"model required"):gsub("[/.]","-"):lower() end)
  end
  local api = customClass(apidef)
  local client = api(apikey, apibase, obj(headers))

  local messages = session and U.getHistory(historyName) or {}
  local data = {
    messages=messages,
    model=model,
    --audio=...
    frequency_penalty=number(frequency_penalty),
    logit_bias=obj(logit_bias),
    logprobs=boolean(logprobs),
    top_logprobs=number(top_logprobs),
    max_tokens=number(max_tokens), --deprecated
    max_completion_tokens=number(max_completion_tokens),
    --modalities={"text","audio"},
    n=number(n),
    presence_penalty=number(presence_penalty),
    reasoning_effort=reasoning_effort,
    repetition_penalty=number(repetition_penalty), --Range: [0, 2] Not available for OpenAI models
    response_format=obj(response_format),
    seed=number(seed),
    --service_tier=
    stop=stop,
    stream=boolean(stream),
    --stream_options={include_usage=true,},
    temperature=number(temperature),
    top_k=number(top_k), --Range: [1, Infinity] Not available for OpenAI models
    top_p=number(top_p),
    verbosity=verbosity,
  }
  if extra_body then
    for k,v in pairs(obj(extra_body)) do
      data[k] = v
    end
  end
  if role then
    if messages[1] and messages[1].role=="system" then
      messages[1].content = role
    else
      table.insert(messages, 1, {role="system", content=role})
    end
  end
  table.insert(messages, {role="user", content=context})
  if args and args.hook then
    data = args.hook(client,data) or data
  end
  local CONTROL = 17 --CtrlEnter
  if win.GetKeyState and win.GetKeyState(CONTROL) then -- use completions endpoint
    data.messages = nil
    data.echo = true
    data.prompt = context
    local prompt,suffix = context:match"^(.-)>_<(.-)$"
    if suffix then
      data.prompt, data.suffix = prompt, suffix
    end
    --data.best_of = 2
    client.defaultEndpoint = "/completions"
    if apibase:find"mistral.ai/v1$" or apibase:find"inceptionlabs.ai" then --todo param
      client.defaultEndpoint = "/fim/completions"
    end
  end

  return function (cb)
    local lastIdx = 0
    local sse = {}
    U.setHistory(historyName.."_debug", {sse=sse})
    local chunks, reasoning_chunks, reasoning_details = {}, {}, {}
    local isThinking, reasoning_key
    local function ln() return (chunks[1] or isThinking) and "\n\n" or "" end
    local function notNull (v)
      return v and v~=U.json.null and v or nil
    end
    local function isInteresting (reason)
      return notNull(reason) and reason~="stop" and reason~="eos" and reason~="" -- together.ai: eos
    end
    local function streamText (text, thinking, idx, chunk_model)
      if thinking then
        if not isThinking then
          isThinking = true
          cb("<think>\n")
          text = text:match"^\n?(.*)"
        end
      else
        if isThinking then
          isThinking = false
          cb("\n</think>\n\n")
        end
      end
      if idx==0 then
        table.insert(thinking and reasoning_chunks or chunks, text)
      end
      cb(text, {model=chunk_model})
    end
    local function processParts (arr, idx, thinking, chunk_model)
      for _,part in ipairs(arr) do
        if part.type=="text" then
          streamText(part.text, thinking, idx, chunk_model)
        elseif part.type=="thinking" then -- Magistral
          reasoning_key = "part.thinking"
          processParts(part.thinking, idx, true, chunk_model)
        else
          cb(ln()..U.formatErrMsg{ statusline="Unknown part type: "..tostring(part.type), data=part }.."\n")
        end
      end
    end
    local response,meta = client:generate(data, function (chunk,err,extra)
      if err then
        cb(ln()); chunks[1] = chunks[1] or ""
        if type(extra)=="string" then
          err = err.." "..extra
        elseif type(extra)=="table" then
          return cb(U.formatErrMsg{ statusline=err, data=extra }.."\n")
        end
        return cb("Error: "..err)
      end
      table.insert(sse,chunk)
      if chunk.error then --OpenRouter
        --le(chunk, "ERROR")
        chunks[1] = chunks[1] or ""
        return cb(ln()..("Error %s: %s"):format(chunk.error.code, chunk.error.message))
      end
      if not notNull(chunk.choices) then
        --le(chunk, "chunk.choices is null")
        return
      end
      local candidate = chunk.choices[1]
      if not candidate then return end
      candidate.index = candidate.index or 0
      if candidate.index~=lastIdx then
        cb("\n\n")
        lastIdx = candidate.index
      end
      local message = candidate.delta
      if notNull(message) then
        local reasoning = message.reasoning or message.reasoning_content
        if notNull(reasoning) and reasoning~="" then
          reasoning_key = reasoning_key or message.reasoning and "reasoning" or "reasoning_content"
          streamText(reasoning, true, candidate.index, chunk.model)
        elseif notNull(message.reasoning_details) and message.reasoning_details[1] then --OpenRouter, MiniMax
          --https://openrouter.ai/docs/guides/best-practices/reasoning-tokens#reasoning_details-array-structure
          --N.B. OpenRouter responses include both the `.reasoning` field and the `.reasoning_details[]` array
          for _,part in ipairs(message.reasoning_details) do
            if part.type=="reasoning.text" then
              streamText(part.text, true, candidate.index, chunk.model)
            elseif part.type=="reasoning.summary" then --todo debug
              local summary = "\n<summary>"..part.summary.."</summary>\n"
              streamText(summary, true, candidate.index, chunk.model)
            else --todo debug
              streamText("<"..part.type..">")
            end
            table.insert(reasoning_details, part)
          end
        elseif notNull(message.refusal) then
          cb("refusal: "..message.refusal)
        elseif notNull(message.content) then
          if #chunks==1 and chunks[1]==message.content then return end -- workaround for qwen-code-cli bug
          if type(message.content)=="table" then
            processParts(message.content, candidate.index, chunk.model)
          else
            streamText(message.content, false, candidate.index, chunk.model)
          end
        --else le(message)
        end
      elseif candidate.text then
        streamText(candidate.text, false, candidate.index, chunk.model)
      --else le(candidate)
      end
      if isInteresting(candidate.finish_reason) then
        cb(ln().."finish_reason: "..candidate.finish_reason)
      end
      if U.utils.check"Esc" then
        table.remove(messages)
        error("interrupted", 0)
      end
      -- Gemini ----------------
      if candidate.index==0 then
        local google = message.extra_content and message.extra_content.google
        if google then
          chunks.thought = chunks.thought or google.thought
          chunks.thought_signature = chunks.thought_signature or google.thought_signature
        end
      end
    end)

    if response==U.restapi.STREAMED then
      local message = {role="assistant"}
      local content = table.concat(chunks)
      if reasoning_details[1] then -- OpenRouter, MiniMax
        message.content = content
        message.reasoning_details = reasoning_details
        assert(not reasoning_key)
      elseif reasoning_key=="part.thinking" then -- Magistral
        message.content={
          {type="thinking",thinking={
            {type="text",text=table.concat(reasoning_chunks)},
          }},
          {type="text",text=table.concat(chunks)},
        }
      else
        message.content = content
        if reasoning_key then message[reasoning_key] = table.concat(reasoning_chunks) end
        if chunks.thought or chunks.thought_signature then -- Gemini
          message.extra_content = {
            google={
              thought=chunks.thought,
              thought_signature=chunks.thought_signature,
            }
          }
        end
      end
      table.insert(messages, message)
    elseif response then
      local cand1 = response.choices[1]
      local message = cand1.message
      table.insert(messages, message)
      if cand1.text then
        assert(not message.content, "Debug: unexpected response")
        message.content = cand1.text
      end
      for i,candidate in ipairs(response.choices) do
        isThinking = false
        if i>1 then cb("\n\n") end
        local reasoning = candidate.message.reasoning or candidate.message.reasoning_content
        if notNull(reasoning) then
          cb("<think>\n" .. reasoning:match"^\n?(.-)\n*$" .. "\n</think>\n\n")
        elseif notNull(message.reasoning_details) and message.reasoning_details[1] then --OpenRouter, MiniMax
          local part = message.reasoning_details[1]
          reasoning = part.text or part.summary or part.type
          cb("<think>\n" .. reasoning:match"^\n?(.-)\n*$" .. "\n</think>\n\n")
          assert(not message.reasoning_details[2]) --todo debug
        end
        local content = candidate.text or candidate.message.content
        if notNull(content) then
          if type(content)=="table" then
            processParts(content)
          else
            cb(content)
          end
        end
        local refusal = candidate.message.refusal
        if notNull(refusal) then
          cb("refusal: "..refusal)
        end
        if isInteresting(candidate.finish_reason) then
          cb("\nfinish_reason: "..candidate.finish_reason)
        end
      end
    else
      table.remove(messages)
      if not chunks[1] then
        cb(ln()..U.formatErrMsg(meta,"\n\n",getMsgText))
      end
    end
    meta.sse = next(sse) and sse
    meta.data = meta.data or response
    U.setHistory(historyName.."_debug", meta)
  end
end,

getModels
