name = "Cohere"
url = "https://coral.cohere.com/"
--https://dashboard.cohere.com/playground/chat

local U = ...
if not U.restapi then return end
exe = true

historyName = name

predefined = {
  apikey=win.GetEnv"COHERE_API_KEY",
  model={
    --https://cohere.com/command
    --"command-a-reasoning-08-2025",--https://cohere.com/blog/command-a-reasoning
    "command-a-vision-07-2025",--https://cohere.com/blog/command-a-vision
    "command-a-03-2025",--https://cohere.com/blog/command-a
    "command-r-plus-08-2024", "command-r-08-2024", --https://cohere.com/blog/command-series-0824
    "command-r7b-12-2024", --https://cohere.com/blog/command-r7b
    "command-r7b-arabic-02-2025", --https://cohere.com/blog/command-r7b-arabic
    --"command-r-plus", --https://cohere.com/blog/command-r-plus-microsoft-azure
    --"command-r", --https://cohere.com/blog/command-r
    --"command", "command-light", "command-nightly", "command-light-nightly",
    --https://cohere.com/research/aya
    --https://cohere.com/blog/aya
    "c4ai-aya-expanse-32b", "c4ai-aya-expanse-8b", --https://cohere.com/blog/aya-expanse-connecting-our-world
    "c4ai-aya-vision-32b", "c4ai-aya-vision-8b", --https://cohere.com/blog/aya-vision
  },
  connectors={--test
    "",
    '{{id: "web-search"}}',
    '{{id: "web-search", options: site: "example.com"}}',
  },
  response_format={"", 'type: "json_object"'},
  safety_mode={"", "CONTEXTUAL", "STRICT", "NONE"},
}

hidden = {
  "top_k", "top_p", "seed", "stop_sequences", "frequency_penalty", "presence_penalty",
  "force_single_step", "response_format",
}

local function getMsgText (data)
  return data.message
end

--https://docs.cohere.com/changelog
--https://docs.cohere.com/reference/chat-v1
return function (session, stream, context, apikey, model, role, connectors, --[[search_queries_only,]] temperature,
                 max_tokens, max_input_tokens, top_k, top_p, seed, stop_sequences, frequency_penalty, presence_penalty,
                 raw_prompting, force_single_step, response_format, safety_mode)
  local client = U.restapi.LinesStream(apikey, "https://api.cohere.ai/v1", {Accept="application/json"})
  function client:is_valid (parsed) --luacheck: ignore 212/self
    return parsed.event_type
  end
  function client:is_valid_final (parsed) --luacheck: ignore 212/self
    return parsed.text
  end

  local history = session and U.getHistory(historyName) or {}
  local boolean,number,obj = U.boolean, U.number, U.obj
  local data = {
    message=context,
    stream=boolean(stream),
    model=model,
    preamble=role,
    chat_history=history[1] and history or nil,
    --conversation_id
    --prompt_truncation
    connectors=obj(connectors),
    --search_queries_only=boolean(search_queries_only),
    --documents
    --citation_quality
    temperature=number(temperature), -- 0.3
    max_tokens=number(max_tokens),
    max_input_tokens=number(max_input_tokens),
    k=number(top_k),
    p=number(top_p),
    seed=number(seed),
    stop_sequences=obj(stop_sequences),
    frequency_penalty=number(frequency_penalty),
    presence_penalty=number(presence_penalty),
    raw_prompting=boolean(raw_prompting),
    --tools
    --tool_results
    force_single_step=boolean(force_single_step),
    response_format=obj(response_format),
    safety_mode=safety_mode,
  }
  return function (cb)
    local sse = {}
    U.setHistory(historyName.."_debug", {sse=sse})
    local finalResponse, streamed
    local function ln() return streamed and "\n\n" or "" end
    local response,meta = client:generate("/chat", data, function (chunk,err,extra)
      if err then
        cb(ln()); streamed = true
        if type(extra)=="string" then
          err = err.." "..extra
        elseif type(extra)=="table" then
          return cb(U.formatErrMsg{ statusline=err, data=extra }.."\n")
        end
        return cb("Error: "..err)
      end
      table.insert(sse,chunk)
      if chunk.event_type=="stream-start" then --luacheck: ignore 542
        -- noop
      elseif chunk.event_type=="text-generation" then
        cb(assert(chunk.text, "text expected"),nil)
        streamed = true
      elseif chunk.event_type=="stream-end" then
        finalResponse = assert(chunk.response, "response expected")
      elseif chunk.event_type=="search-queries-generation"
          or chunk.event_type=="search-results"
          or chunk.event_type=="citation-generation" then --luacheck: ignore 542
        --le(chunk)
      elseif chunk.event_type then
        error("Unexpected event type: "..tostring(chunk.event_type))
      else
        error("Unexpected response:\n"..U.formatJson(chunk))
      end
      if U.utils.check"Esc" then
        error("interrupted", 0)
      end
    end)

    if response then
      if not finalResponse then
        if response==U.restapi.STREAMED then
          cb(ln().."Error: stream-end event missed")
        else
          cb(response.text)
          finalResponse = response
        end
      end
      if finalResponse then
        if finalResponse.finish_reason~="COMPLETE" then
          cb(ln().."finish_reason: "..finalResponse.finish_reason)
        end
        U.setHistory(historyName, assert(finalResponse.chat_history, "chat_history expected"))
      end
    elseif not streamed then
      cb(ln()..U.formatErrMsg(meta,"\n\n",getMsgText))
    end
    meta.sse = next(sse) and sse
    meta.data = meta.data or response
    U.setHistory(historyName.."_debug", meta)
  end
end,

--https://docs.cohere.com/reference/list-models
function (data) -- getModels
  local client = U.restapi.LinesStream(data.apikey, "https://api.cohere.ai/v1")
  client.formatErr = function (errMeta)
    return U.formatErrMsg(errMeta,"\n\1\n",getMsgText)
  end
  return assert(client:models("/models?endpoint=chat"))
end
