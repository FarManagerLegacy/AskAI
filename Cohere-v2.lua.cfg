name = "Cohere v2"
url = "https://coral.cohere.com/"
--https://dashboard.cohere.com/playground/chat

local U = ...
if not U.restapi then return end
exe = true

historyName = name

predefined = {
  apikey=win.GetEnv"COHERE_API_KEY",
  model={
    --https://cohere.com/command
    "command-a-reasoning-08-2025",--https://cohere.com/blog/command-a-reasoning
    "command-a-vision-07-2025",--https://cohere.com/blog/command-a-vision
    "command-a-03-2025",--https://cohere.com/blog/command-a
    "command-r-plus-08-2024", "command-r-08-2024", --https://cohere.com/blog/command-series-0824
    "command-r7b-12-2024", --https://cohere.com/blog/command-r7b
    "command-r7b-arabic-02-2025", --https://cohere.com/blog/command-r7b-arabic
    --"command-r-plus", --https://cohere.com/blog/command-r-plus-microsoft-azure
    --"command-r", --https://cohere.com/blog/command-r
    --"command", "command-light", "command-nightly", "command-light-nightly",
    --https://cohere.com/research/aya
    --https://cohere.com/blog/aya
    "c4ai-aya-expanse-32b", "c4ai-aya-expanse-8b", --https://cohere.com/blog/aya-expanse-connecting-our-world
    "c4ai-aya-vision-32b", "c4ai-aya-vision-8b", --https://cohere.com/blog/aya-vision
 },
 reasoning={"", "enabled", "disabled"},
 response_format={"", 'type: "json_object"'},
 safety_mode={"", "CONTEXTUAL", "STRICT", "NONE"},
}

local function getMsgText (data)
  return data.message
end

--https://docs.cohere.com/changelog
--https://docs.cohere.com/reference/chat-v1
return function (session, stream, context, apikey, model, role, response_format, safety_mode, max_tokens,
                 stop_sequences, temperature, seed, frequency_penalty, presence_penalty, top_k, top_p,
                 reasoning, token_budget, raw_prompting)
  local client = U.restapi.LinesStream(apikey, "https://api.cohere.ai/v2", {Accept="application/json"})
  function client:is_valid (parsed) --luacheck: ignore 212/self
    return parsed.type
  end
  function client:is_valid_final (parsed) --luacheck: ignore 212/self
    return parsed.message
  end

  local messages = session and U.getHistory(historyName) or {}
  local boolean,number,obj = U.boolean, U.number, U.obj
  local data = {
    messages=messages,
    stream=boolean(stream),
    model=model,
    --tools
    --documents
    --citation_options
    response_format=obj(response_format),
    safety_mode=safety_mode,
    max_tokens=number(max_tokens),
    stop_sequences=obj(stop_sequences),
    temperature=number(temperature), -- 0.3
    seed=number(seed),
    frequency_penalty=number(frequency_penalty),
    presence_penalty=number(presence_penalty),
    k=number(top_k),
    p=number(top_p),
    --log_probs
    --tool_choice
    thinking={
      type=reasoning,
      token_budget=number(token_budget)},
    raw_prompting=boolean(raw_prompting),
    --strict_tools
  }
  if role then
    if messages[1] and messages[1].role=="system" then
      messages[1].content = role
    else
      table.insert(messages, 1, {role="system", content=role})
    end
  end
  table.insert(messages, {role="user", content=context})
  return function (cb)
    local chunks,thinking = {},false
    local function ln() return chunks[1] and "\n\n" or "" end
    local response,meta = client:generate("/chat", data, function (chunk,err,extra)
      if err then
        cb(ln()); chunks[1] = chunks[1] or ""
        if type(extra)=="string" then
          err = err.." "..extra
        elseif type(extra)=="table" then
          return cb(U.formatErrMsg({ statusline=err, data=extra }, "\n", getMsgText).."\n")
        end
        cb("Error: "..err)
      elseif chunk.type=="message-start"
          or chunk.type=="content-start"
          or chunk.type=="content-end" then --luacheck: ignore 542
        -- noop
      elseif chunk.type=="content-delta" then
        local content = chunk.delta.message.content
        if content.thinking then
          if not thinking then
            thinking = true
            cb("<think>\n")
          end
          chunks[1] = chunks[1] or ""
          cb(content.thinking)
        else
          if thinking then
            thinking = false
            cb("\n</think>\n\n")
          end
          assert(content.text, "text expected")
          table.insert(chunks, content.text)
          cb(content.text)
        end
      elseif chunk.type=="message-end" then
        if chunk.delta.finish_reason~="COMPLETE" then
          cb(ln().."finish_reason: "..chunk.delta.finish_reason)
        end
      elseif chunk.type then
        error("Unexpected event type: "..tostring(chunk.type))
      else
        error("Unexpected response:\n"..U.formatJson(chunk))
      end
      if U.check"Esc" then error("interrupted", 0) end
    end)

    if response==U.restapi.STREAMED then
      table.insert(messages, {role="assistant", content=table.concat(chunks)})
    elseif response then
      table.insert(messages, response.message)
      for _,part in ipairs(response.message.content) do
        if part.type=="thinking" then
          cb("<think>\n"..part.thinking.."\n</think>\n\n")
        else
          assert(part.type=="text", part.type)
          cb(part.text)
        end
      end
      if response.finish_reason~="COMPLETE" then
        cb(ln().."finish_reason: "..response.finish_reason)
      end
    else
      table.remove(messages)
      if not chunks[1] then
        cb(ln()..U.formatErrMsg(meta,"\n\n",getMsgText))
      end
    end
    cb()
  end
end,

--https://docs.cohere.com/reference/list-models
function (data) -- getModels
  local client = U.restapi.LinesStream(data.apikey, "https://api.cohere.ai/v2")
  client.formatErr = function (errMeta)
    return U.formatErrMsg(errMeta,"\n\1\n",getMsgText)
  end
  return assert(client:models("/models?endpoint=chat"))
end
