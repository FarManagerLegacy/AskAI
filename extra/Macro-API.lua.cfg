--https://farmanagerlegacy.github.io/macro-api/
name = "MacroAPI assistant"
local applyParams = _import "openai.lua.cfg"
if not exe then return end
local U = ...

predefined = {
  model={
    "claude-3-5-sonnet-20240620",
    "gpt-3.5-turbo",
    "gpt-4",
    "gpt-4o",
    "gpt-4o-mini",
    "gpt-4-32k",
    "gpt-4-1106-preview",
    "gpt-4-turbo-preview",
    "meta-llama-3-70b-instruct",
    "meta-llama-3-8b-instruct",
    "mixtral-8x7b-instruct-v0.1",
    "groq-llama-3.1-70b-versatile",
    "groq-llama3-70b-8192",
    "groq-llama-3.1-8b-instant",
    "groq-llama3-groq-70b-8192-tool-use-preview",
  }
}

--https://markprompt.com/docs/api/chat
return function (session, context, model, temperature, top_p, frequency_penalty, presence_penalty, max_tokens, role)
  return applyParams {
    session, true, context,
    apibase="https://api.markprompt.com",
    model=model,
    temperature=temperature,
    role=role,
    headers={origin="https://farmanagerlegacy.github.io", ["x-markprompt-api-version"]="2024-05-21"},
    hook=function (client, data)
      client.defaultEndpoint = "/chat"
      data.topP = U.number(top_p)
      data.frequencyPenalty = U.number(frequency_penalty)
      data.presencePenalty = U.number(presence_penalty)
      data.maxTokens = U.number(max_tokens)
      data.projectKey="kcEcTe2Xg9ghaQyyEy7bbIAzPZe6plbN"
      --data.apiUrl="https://api.markprompt.com"
      --data.iDontKnowMessage="Sorry, I am not sure how to answer that."
    end
  }
end
